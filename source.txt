# -*- coding: utf-8 -*-
"""project_source.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aphN78jzYCWfU4XPb1IYqzuW3B1XLvki
"""

# Install PySpark
!pip install pyspark
!pip show pyspark

# Import SparkSession
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, DoubleType, StringType,DateType,IntegerType

# Create a SparkSession
spark = SparkSession.builder.master("local").getOrCreate()

!wget "https://github.com/var595/DataForbigdata/raw/main/weatherAUS.csv" -O weatherAUS.csv

schema = StructType([
    StructField("Date", DateType(), True),
    StructField("Location", StringType(), True),
    StructField("MinTemp", DoubleType(), True),
    StructField("MaxTemp", DoubleType(), True),
    StructField("Rainfall", DoubleType(), True),
    StructField("Evaporation", DoubleType(), True),
    StructField("Sunshine", DoubleType(), True),
    StructField("WindGustDir", StringType(), True),
    StructField("WindGustSpeed", DoubleType(), True),
    StructField("WindDir9am", StringType(), True),
    StructField("WindDir3pm", StringType(), True),
    StructField("WindSpeed9am", DoubleType(), True),
    StructField("WindSpeed3pm", DoubleType(), True),
    StructField("Humidity9am", DoubleType(), True),
    StructField("Humidity3pm", DoubleType(), True),
    StructField("Pressure9am", DoubleType(), True),
    StructField("Pressure3pm", DoubleType(), True),
    StructField("Cloud9am", DoubleType(), True),
    StructField("Cloud3pm", DoubleType(), True),
    StructField("Temp9am", DoubleType(), True),
    StructField("Temp3pm", DoubleType(), True),
    StructField("RainToday", StringType(), True),
    StructField("RainTomorrow", StringType(), True)
])

df = spark.read.csv("weatherAUS.csv", header=True, schema=schema)
df.show()
df.printSchema()

import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
pd.set_option('display.max_columns', None)
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

from sklearn import preprocessing, impute
from sklearn.utils import resample
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler
from sklearn.decomposition import KernelPCA, PCA
from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split, RandomizedSearchCV

from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc
from sklearn.metrics import f1_score, classification_report, confusion_matrix

# Perform groupby and aggregation to count occurrences
rain_today_counts = df.groupBy('RainToday').count().collect()
rain_tomorrow_counts = df.groupBy('RainTomorrow').count().collect()

# Extract counts and labels for plotting
labels_today = [row['RainToday'] for row in rain_today_counts]
counts_today = [row['count'] for row in rain_today_counts]

labels_tomorrow = [row['RainTomorrow'] for row in rain_tomorrow_counts]
counts_tomorrow = [row['count'] for row in rain_tomorrow_counts]

# Create a local matplotlib figure and axes
fig, axes = plt.subplots(1, 2, figsize=(15, 8))

# Plot RainToday counts
axes[0].bar(labels_today, counts_today)
axes[0].set_title('Today')
axes[0].set_xlabel('Rain')
axes[0].set_ylabel('Count')

# Annotate bars with counts
for i, v in enumerate(counts_today):
    axes[0].text(i, v + 10, str(v), ha='center')

# Plot RainTomorrow counts
axes[1].bar(labels_tomorrow, counts_tomorrow)
axes[1].set_title('Tomorrow')
axes[1].set_xlabel('Rain')
axes[1].set_ylabel('Count')

# Annotate bars with counts
for i, v in enumerate(counts_tomorrow):
    axes[1].text(i, v + 10, str(v), ha='center')

# Adjust layout and display the plot
plt.tight_layout()
plt.show()

# Assuming 'df' is your PySpark DataFrame containing the rain data

# Register the DataFrame as a temporary view to use SQL queries
df.createOrReplaceTempView("weather_data")

# Perform a SQL query to compute the custom cross-tabulation
crosstab_df = spark.sql("""
    SELECT
        RainToday,
        SUM(CASE WHEN RainTomorrow = 'NA' THEN 1 ELSE 0 END) AS NA,
        SUM(CASE WHEN RainTomorrow = 'Yes' THEN 1 ELSE 0 END) AS Yes,
        SUM(CASE WHEN RainTomorrow = 'No' THEN 1 ELSE 0 END) AS No
    FROM weather_data
    GROUP BY RainToday
    ORDER BY RainToday
""")

# Display the result
crosstab_df.show()

# Convert the PySpark DataFrame to Pandas DataFrame for display purposes (optional)
result_df = crosstab_df.toPandas()
print(result_df)

# Assuming 'df' is your PySpark DataFrame containing the rain data

# Filter out rows where RainToday or RainTomorrow is null or 'NA'
filtered_df =df.filter((df['RainToday'].isNotNull()) & (df['RainToday'] != 'NA') &
                        (df['RainTomorrow'].isNotNull()) & (df['RainTomorrow'] != 'NA'))

# Register the filtered DataFrame as a temporary view to use SQL queries
filtered_df.createOrReplaceTempView("weather_data_filtered")

# Perform a SQL query to compute the custom cross-tabulation
crosstab_df = spark.sql("""
    SELECT
        RainToday,
        SUM(CASE WHEN RainTomorrow = 'NA' THEN 1 ELSE 0 END) AS NA,
        SUM(CASE WHEN RainTomorrow = 'Yes' THEN 1 ELSE 0 END) AS Yes,
        SUM(CASE WHEN RainTomorrow = 'No' THEN 1 ELSE 0 END) AS No
    FROM weather_data_filtered
    GROUP BY RainToday
    ORDER BY RainToday
""")

# Display the result
crosstab_df.show()

# Convert the PySpark DataFrame to Pandas DataFrame for display purposes (optional)
result_df = crosstab_df.toPandas()
print(result_df)

# Perform groupby and aggregation to count occurrences
rain_today_counts = filtered_df.groupBy('RainToday').count().collect()
rain_tomorrow_counts = filtered_df.groupBy('RainTomorrow').count().collect()

# Extract counts and labels for plotting
labels_today = [row['RainToday'] for row in rain_today_counts]
counts_today = [row['count'] for row in rain_today_counts]

labels_tomorrow = [row['RainTomorrow'] for row in rain_tomorrow_counts]
counts_tomorrow = [row['count'] for row in rain_tomorrow_counts]

# Create a local matplotlib figure and axes
fig, axes = plt.subplots(1, 2, figsize=(15, 8))

# Plot RainToday counts
axes[0].bar(labels_today, counts_today)
axes[0].set_title('Today')
axes[0].set_xlabel('Rain')
axes[0].set_ylabel('Count')

# Annotate bars with counts
for i, v in enumerate(counts_today):
    axes[0].text(i, v + 10, str(v), ha='center')

# Plot RainTomorrow counts
axes[1].bar(labels_tomorrow, counts_tomorrow)
axes[1].set_title('Tomorrow')
axes[1].set_xlabel('Rain')
axes[1].set_ylabel('Count')

# Annotate bars with counts
for i, v in enumerate(counts_tomorrow):
    axes[1].text(i, v + 10, str(v), ha='center')

# Adjust layout and display the plot
plt.tight_layout()
plt.show()

# Assuming 'weather_df' is your PySpark DataFrame containing the weather data

# Get the column names for categorical and numerical features
categorical_features = [col for col, dtype in df.dtypes if dtype == 'string']
numerical_features = [col for col, dtype in df.dtypes if dtype in ['double', 'float']]

# Print the number of numerical and categorical features
print('Number of numerical independent features: ', len(numerical_features))
print('Number of categorical independent features: ', len(categorical_features))

import scipy.stats as stats

# Define a function to plot histogram and Q-Q plot for a numerical column
def plot_numerical_distribution(col):
    # Extract the column data and convert it to a Pandas Series for plotting
    column_data =df.select(col).dropna().toPandas()[col]

    # Create subplots for the current column
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))

    # Plot histogram with KDE
    sns.histplot(column_data, ax=axes[0], kde=True)
    axes[0].set_title(f'{col} - Histogram')

    # Plot Q-Q plot (Probability plot)
    stats.probplot(column_data, dist='norm', plot=axes[1])
    axes[1].set_title(f'{col} - Q-Q Plot')

    # Display the plot
    plt.tight_layout()
    plt.show()

# Iterate over each numerical feature and plot its distribution
numerical_features = [col for col, dtype in df.dtypes if dtype in ['double', 'float']]

for col in numerical_features:
    plot_numerical_distribution(col)

# Calculate Spearman correlation matrix
correlation_matrix =df.select(numerical_features).toPandas().corr(method='spearman')

# Create a mask for the upper triangle of the heatmap
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))

# Plot the correlation heatmap using Seaborn and Matplotlib
plt.figure(figsize=(16, 10))
sns.heatmap(correlation_matrix, annot=True, mask=mask)
plt.xticks(rotation=45)
plt.title('Spearman Correlation Heatmap')
plt.show()

from pyspark.sql.functions import col, mean
from pyspark.sql import functions as F

# List of numerical column names (double type)
numerical_columns = [col_name for col_name, data_type in df.dtypes if data_type == 'double']

# Calculate the mean values for each numerical column
mean_values = filtered_df.select([mean(col(col_name)).alias(col_name) for col_name in numerical_columns]).collect()[0].asDict()

# Replace null values with mean values in each numerical column
for col_name in numerical_columns:
    filtered_df = filtered_df.withColumn(col_name, F.when(df[col_name].isNull(), mean_values[col_name]).otherwise(df[col_name]))

# first_column_name = filtered_df.columns[0]
# filtered_df.drop(first_column_name)
# Show the updated DataFrame with replaced null values
filtered_df.show()

# COMMAND ----------

df = df.filter(col("RainTomorrow") != "NA")
print(df.groupBy("RainTomorrow").count().show())

# COMMAND ----------

df = df.filter(col("RainToday") != "NA")
print(df.groupBy("RainTomorrow").count().show())

# COMMAND ----------
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, DoubleType, StringType,DateType,IntegerType
from pyspark.sql.functions import col, year, isnan, when, count
train_val_df, test_df = df.randomSplit([0.8, 0.2], seed=42)
df = df.withColumn("Year", year(col("Date")))
train_df = df.filter(col("Year") < 2015)
test_df = df.filter(col("Year") >= 2015)

target_col = "RainTomorrow"

# Define input columns; exclude the first and the last column
# Assuming here that the first column might be a non-input like an ID or Date which you typically don't use for training
input_cols = train_df.columns[1:-1]
# Prepare training input and target DataFrames
train_inputs = train_df.select(input_cols)
train_target = train_df.select(target_col)
print(train_target.groupBy("RainTomorrow").count().show())
# Prepare test input and target DataFrames
test_inputs = test_df.select(input_cols)
test_targets = test_df.select(target_col)

# Dropping the 'RainTomorrow' column from the DataFrame
train_inputs = train_inputs.drop("RainTomorrow")
test_inputs = test_inputs.drop("RainTomorrow")

print("Train Inputs Schema:")
train_inputs.printSchema()

print("Train Target Schema:")
train_target.printSchema()

from pyspark.sql.types import StringType, IntegerType, DoubleType, FloatType, DecimalType, LongType
def get_column_types(df):
    numeric_types = [IntegerType, DoubleType, FloatType, DecimalType, LongType]
    categorical_types = [StringType]

    numeric_cols = [field.name for field in df.schema.fields if any([isinstance(field.dataType, t) for t in numeric_types])]
    categorical_cols = [field.name for field in df.schema.fields if any([isinstance(field.dataType, t) for t in categorical_types])]

    return numeric_cols, categorical_cols

# Assuming `train_inputs` is already defined
numeric_cols, categorical_cols = get_column_types(train_inputs)
print(numeric_cols)
print()
print(categorical_cols)

#

# COMMAND ----------

print(train_inputs.printSchema())

# COMMAND ----------

from pyspark.sql.functions import mean, col

# Assuming 'train_inputs', 'test_inputs' are your DataFrames
# and 'numeric_cols' is a list of your numeric columns.

# Calculate means for all numeric columns in the train dataset
means = train_inputs.select([mean(c).alias(c) for c in numeric_cols]).collect()[0].asDict()

# Show calculated means (optional)
print("Column means:", means)

# COMMAND ----------

# Fill missing values in train, validation, and test datasets
train_inputs = train_inputs.fillna(means)
test_inputs = test_inputs.fillna(means)

# COMMAND ----------

# Check for remaining nulls in each DataFrame
for df_name, df in zip(["Train",  "Test"], [train_inputs,  test_inputs]):
    null_counts = df.select([count(when(col(c).isNull(), c)).alias(c) for c in numeric_cols]).collect()[0].asDict()
    print(f"Remaining NULLs in {df_name} dataset:", null_counts)

# COMMAND ----------

from pyspark.sql.functions import min, max

# Compute the minimum and maximum values for each numeric column
min_values = train_inputs.select([min(c).alias(c) for c in numeric_cols]).collect()[0].asDict()
max_values = train_inputs.select([max(c).alias(c) for c in numeric_cols]).collect()[0].asDict()

print("Minimum values:", min_values)
print("Maximum values:", max_values)

# COMMAND ----------

from pyspark.ml.feature import MinMaxScaler, VectorAssembler
from pyspark.ml import Pipeline
# Assemble numeric columns into a vector
assembler = VectorAssembler(inputCols=numeric_cols, outputCol="features")

# COMMAND ----------

# Initialize the MinMaxScaler
scaler = MinMaxScaler(inputCol="features", outputCol="scaled_features")

# Define the pipeline that includes both assembling and scaling steps
pipeline = Pipeline(stages=[assembler, scaler])

# Fit the pipeline on the training data
scalerModel = pipeline.fit(train_inputs)

# Transform the data using the model
train_inputs = scalerModel.transform(train_inputs)
test_inputs = scalerModel.transform(test_inputs)

# COMMAND ----------

# Show scaled features
train_inputs.select("scaled_features").show(truncate=False)

# Optionally, to split the vector back to columns (not always necessary, depends on further usage)
from pyspark.sql.functions import col, udf
from pyspark.sql.types import DoubleType

# Example to extract the first scaled feature into a new column
get_scaled_value = udf(lambda vec: float(vec[0]), DoubleType())
train_inputs = train_inputs.withColumn('MinTemp_scaled', get_scaled_value(col('scaled_features')))

# COMMAND ----------

# You could calculate the min and max from the scaled data if necessary
scaled_min = train_inputs.select([min(col("scaled_features")).alias("min")]).collect()[0]["min"]
scaled_max = train_inputs.select([max(col("scaled_features")).alias("max")]).collect()[0]["max"]

print("Scaled min values: ", scaled_min)
print("Scaled max values: ", scaled_max)

# COMMAND ----------

from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler
from pyspark.ml import Pipeline
# Create a list of string indexers
indexers = [StringIndexer(inputCol=c, outputCol=f"{c}_index", handleInvalid='keep') for c in categorical_cols]
# Create a OneHotEncoder
encoders = [OneHotEncoder(inputCol=f"{c}_index", outputCol=f"{c}_encoded") for c in categorical_cols]
# Assemble all encoded features along with any numeric features you might want to include
assembler = VectorAssembler(inputCols=[f"{c}_encoded" for c in categorical_cols] + numeric_cols, outputCol="features_vector")
# Create a Pipeline
pipeline = Pipeline(stages=indexers + encoders + [assembler])

# Fit the pipeline to the training data
pipeline_model = pipeline.fit(train_inputs)

# Transform the data
train_inputs = pipeline_model.transform(train_inputs)
test_inputs = pipeline_model.transform(test_inputs)

# COMMAND ----------

from pyspark.sql.functions import monotonically_increasing_id

# Add a column of unique IDs
train_target = train_target.withColumn("RainTomorrow", when(col("RainTomorrow") == "Yes", 1).otherwise(0))
train_inputs = train_inputs.withColumn("id", monotonically_increasing_id())
train_target = train_target.withColumn("id", monotonically_increasing_id())

# Now join the DataFrames using the id column
train_df = train_inputs.join(train_target, "id", "inner").drop("id")
print(train_df.groupBy("RainTomorrow").count().show())

# COMMAND ----------

from pyspark.sql.functions import monotonically_increasing_id
test_targets = test_targets.withColumn("RainTomorrow", when(col("RainTomorrow") == "Yes", 1).otherwise(0))
# Add a column of unique IDs
test_inputs = test_inputs.withColumn("id", monotonically_increasing_id())
test_targets = test_targets.withColumn("id", monotonically_increasing_id())

# Now join the DataFrames using the id column
test_df = test_inputs.join(test_targets, "id", "inner").drop("id")
print(test_df.groupBy("RainTomorrow").count().show())

# COMMAND ----------

train_df.select("features_vector").show(truncate=False)

# COMMAND ----------

print(train_df.select("features_vector"))



from pyspark.sql.functions import udf
from pyspark.ml.linalg import VectorUDT, DenseVector
from pyspark.sql.types import ArrayType, FloatType
import numpy as np
# Define a UDF to convert SparseVector to a list of floats (Dense format)
def sparse_to_dense_list(vector):
    return DenseVector(vector).toArray().tolist()

# Register the UDF
sparse_to_dense_list_udf = udf(sparse_to_dense_list, ArrayType(FloatType()))

# Apply the UDF to convert the vectors
df_dense = train_df.withColumn("dense_vector", sparse_to_dense_list_udf("features_vector"))

# Now you can safely convert this to a Pandas DataFrame
pandas_df = df_dense.select("dense_vector").toPandas()

# Convert the list of floats to a numpy array
X_train = np.array(pandas_df['dense_vector'].tolist())

from pyspark.sql.functions import udf
from pyspark.ml.linalg import VectorUDT, DenseVector
from pyspark.sql.types import ArrayType, FloatType
import numpy as np
# Define a UDF to convert SparseVector to a list of floats (Dense format)
def sparse_to_dense_list(vector):
    return DenseVector(vector).toArray().tolist()

# Register the UDF
sparse_to_dense_list_udf = udf(sparse_to_dense_list, ArrayType(FloatType()))

# Apply the UDF to convert the vectors
df_dense = test_df.withColumn("dense_vector", sparse_to_dense_list_udf("features_vector"))

# Now you can safely convert this to a Pandas DataFrame
pandas_df = df_dense.select("dense_vector").toPandas()

# Convert the list of floats to a numpy array
X_test = np.array(pandas_df['dense_vector'].tolist())

# ANN Class using PySpark
class ANN:
    def __init__(self, input_size, hidden_size, output_size):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        return
        # Initialize weights and biases randomly
        self.weights_input_hidden = np.random.rand(self.input_size, self.hidden_size) - 0.5
        self.weights_hidden_output = np.random.rand(self.hidden_size, self.output_size) - 0.5
        self.biases_hidden = np.random.rand(1, self.hidden_size) - 0.5
        self.biases_output = np.random.rand(1, self.output_size) - 0.5

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def sigmoid_derivative(self, x):
        sig = self.sigmoid(x)
        return sig * (1 - sig)

    def forward_propagation(self, x):
        hidden_layer = self.sigmoid(np.dot(x, self.weights_input_hidden) + self.biases_hidden)
        output_layer = self.sigmoid(np.dot(hidden_layer, self.weights_hidden_output) + self.biases_output)
        return output_layer

    def backward_propagation(self, x, y_true, learning_rate=0.1):
        hidden_layer = self.sigmoid(np.dot(x, self.weights_input_hidden) + self.biases_hidden)
        output_layer = self.sigmoid(np.dot(hidden_layer, self.weights_hidden_output) + self.biases_output)

        # Calculate error and deltas
        error = 0.5 * np.sum(np.power(output_layer - y_true, 2))
        output_delta = (output_layer - y_true) * self.sigmoid_derivative(output_layer)
        hidden_delta = np.dot(output_delta, self.weights_hidden_output.T) * self.sigmoid_derivative(hidden_layer)

        # Update weights and biases
        self.weights_hidden_output -= learning_rate * np.dot(hidden_layer.T, output_delta)
        self.biases_output -= learning_rate * np.sum(output_delta, axis=0)
        self.weights_input_hidden -= learning_rate * np.dot(x.T, hidden_delta)
        self.biases_hidden -= learning_rate * np.sum(hidden_delta, axis=0)

        return error

    def train(self, data_rdd, num_epochs=50, learning_rate=0.1, log_file="training_log.txt"):
        f=0
        with open(log_file, "w") as log:
            log.write("Experiment Number(iteration) ,Parameters Chosen , Results\n")
            epoch=0
            while f==0:
                # Calculate average error for the epoch
                if f==0:
                  continue
                total_error = data_rdd.map(lambda data: self.backward_propagation(data[0], data[1], learning_rate)).sum()
                avg_error = total_error / data_rdd.count()

                # Calculate dynamic training results (e.g., accuracy, RMSE)
                train_accuracy, train_rmse = self.evaluate_metrics(data_rdd, "train")
                test_accuracy, test_rmse = self.evaluate_metrics(test_rdd, "test")

                # Output training progress for the epoch
                log.write(f"Iteration {epoch + 1}\n")
                log.write(f"Parameters: Number of layers = {len(self.hidden_size)}, Neurons = {tuple(self.hidden_size)}, Error Function = RMSE, Regularization Parameter\n")
                log.write(f"Results: Train/Test Split = 80:20, Training Accuracy = {train_accuracy:.2%}, Test Accuracy = {test_accuracy:.2%}, Training RMSE = {train_rmse:.2f}, Test RMSE = {test_rmse:.2f}\n")
                log.write("\n")
                epoch+=1
                if epoch==num_epochs:
                  break

    def evaluate_metrics(self, data_rdd, data_type="train"):
        predictions = data_rdd.map(lambda data: (self.forward_propagation(data[0]), data[1]))
        if data_type == "train":
            actual_values = data_rdd.map(lambda data: data[1])
        else:
            actual_values = data_rdd.map(lambda data: data[1])

        predictions_and_actual = predictions.zip(actual_values)
        num_samples = predictions_and_actual.count()

        correct_predictions = predictions_and_actual.filter(lambda x: (x[0] >= 0.5 and x[1] == 1) or (x[0] < 0.5 and x[1] == 0)).count()
        accuracy = correct_predictions / num_samples

        squared_errors = predictions_and_actual.map(lambda x: (x[0] - x[1]) ** 2)
        mean_squared_error = squared_errors.sum() / num_samples
        root_mean_squared_error = np.sqrt(mean_squared_error)

        return accuracy, root_mean_squared_error

# Instantiate the ANN model
input_size = 119
hidden_size = [8, 8, 4]
output_size = 2
ann_model = ANN(input_size, hidden_size, output_size)

# Convert training and test data to RDD format
train_rdd = train_df.rdd.map(lambda row: (row["scaled_features"], row["label"]))
test_rdd = test_df.rdd.map(lambda row: (row["scaled_features"], row["label"]))

# Train the ANN model
num_epochs = 50
learning_rate = 0.1
log_file = "training_progress.log"
ann_model.train(train_rdd, num_epochs=num_epochs, learning_rate=learning_rate, log_file=log_file)